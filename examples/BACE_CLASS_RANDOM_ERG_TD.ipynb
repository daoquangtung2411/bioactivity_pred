{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65aae650",
   "metadata": {},
   "source": [
    "## Extended Reduced Graph fingerprint for bioactivity predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3931e9",
   "metadata": {},
   "source": [
    "##### Import libraries\n",
    "\n",
    "- Helper function: load, split dataset, generate fingerprint\n",
    "\n",
    "- Load model from scikit-learn, torch\n",
    "\n",
    "- Load hyperopt module for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706621d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/Documents/AI4H/RTDS/Project/Coding/.venv/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper.load_dataset import load_bace_classification\n",
    "from helper.preprocess import split_train_valid_test\n",
    "from helper.features import smi_erg\n",
    "from helper.cal_metrics import classification_metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from hyperopt import hp, tpe, fmin, Trials, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718e7",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a60163",
   "metadata": {},
   "source": [
    "##### Random Splitting - BACE Classifcation Task\n",
    "\n",
    "- Load, split dataset\n",
    "\n",
    "- Generate fingerprint, defined target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6479585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "bace_class = load_bace_classification()\n",
    "\n",
    "# Split dataset\n",
    "train, valid, test = split_train_valid_test(bace_class)\n",
    "merge = pd.concat((train, valid))\n",
    "\n",
    "# Generate fingerprint\n",
    "train_smis = train['SMILES']\n",
    "valid_smis = valid['SMILES']\n",
    "test_smis = test['SMILES']\n",
    "merge_smis = merge['SMILES']\n",
    "X_train = [smi_erg(smi) for smi in train_smis]\n",
    "X_valid = [smi_erg(smi) for smi in valid_smis]\n",
    "X_test = [smi_erg(smi) for smi in test_smis]\n",
    "X_merge = [smi_erg(smi) for smi in merge_smis]\n",
    "\n",
    "\n",
    "# Target defined\n",
    "y_train = train['Class']\n",
    "y_valid = valid['Class']\n",
    "y_test = test['Class']\n",
    "y_merge = merge['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccac410",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning and model training\n",
    "\n",
    "- Pipeline: Hyperparameter tuning on train and valid set -> Train best params on train + valid -> Test on test set\n",
    "\n",
    "- Model to try:\n",
    "\n",
    "    - Support Vector Machine\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "    - Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa508a9",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c9086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:31<00:00,  3.20trial/s, best loss: -0.8488372093023255]\n",
      "Random Forest results:\n",
      "Best params: {'max_depth': 10, 'max_features': 115, 'min_samples_leaf': 12, 'min_samples_split': 10, 'n_estimators': 45}\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8472  | 0.7895 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8472  | 0.7895 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.8501  | 0.8267 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.8436  | 0.7532 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8477  | 0.7915 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.8687  | 0.7654 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.8222  | 0.8169 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9312  | 0.8758 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.9152  | 0.8715 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "rf_search_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 100, 5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 2, 20, 1)),\n",
    "    'max_features': scope.int(hp.quniform('max_features', 5, 150, 5)),\n",
    "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 2, 20, 1)),\n",
    "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 20, 1))\n",
    "}\n",
    "\n",
    "def rf_objective(params):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth'], \n",
    "        max_features=params['max_features'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        n_jobs=1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_valid_hat)\n",
    "    return -f1\n",
    "\n",
    "best_rf_params = fmin(\n",
    "    fn=rf_objective,\n",
    "    space=rf_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_rf_params = space_eval(rf_search_space, best_rf_params)\n",
    "\n",
    "model = RandomForestClassifier(**best_rf_params)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_train_score = model.predict_proba(X_merge)[:, 1]\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_score = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = classification_metrics(y_merge, y_train_pred, y_train_score)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred, y_test_score)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "print('Random Forest results:')\n",
    "print(f'Best params: {best_rf_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_rf.txt', 'w') as file:\n",
    "    file.write(f'BACE classfication\\n')\n",
    "    file.write('Random Forest results:\\n')\n",
    "    file.write(f'Best params: {best_rf_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15e97d",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e8d1ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:21<00:00,  4.63trial/s, best loss: -0.8202247191011236]\n",
      "Support Vector Machine results:\n",
      "Best params: {'C': 3.490754673267888, 'kernel': 'rbf'}\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8472  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8472  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.8179  | 0.7867 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.8827  | 0.8312 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8516  | 0.8096 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.8946  | 0.8194 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.7994  | 0.8000 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9296  | 0.8859 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.9127  | 0.8471 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "svm_search_space = {\n",
    "    'C': hp.loguniform('C', np.log(0.1), np.log(10)),\n",
    "    # 'epsilon': hp.uniform('epsilon', 0.01, 0.2),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly'])\n",
    "}\n",
    "\n",
    "def svm_objective(params):\n",
    "    model = SVC(C=params['C'], kernel=params['kernel'])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_valid_hat)\n",
    "    return -f1\n",
    "\n",
    "best_svm_params = fmin(\n",
    "    fn=svm_objective,\n",
    "    space=svm_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_svm_params = space_eval(svm_search_space, best_svm_params)\n",
    "\n",
    "model = SVC(**best_svm_params, probability=True)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_train_score = model.predict_proba(X_merge)[:, 1]\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_score = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = classification_metrics(y_merge, y_train_pred, y_train_score)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred, y_test_score)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "print('Support Vector Machine results:')\n",
    "print(f'Best params: {best_svm_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_svm.txt', 'w') as file:\n",
    "    file.write(f'BACE classfication\\n')\n",
    "    file.write('Support Vector Machine results:\\n')\n",
    "    file.write(f'Best params: {best_rf_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fa474",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb31dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.53trial/s, best loss: -0.8390804597701149]\n",
      "XGBoost Classifier results:\n",
      "Best params: {'booster': 'gbtree', 'colsample_bytree': 0.571759606287719, 'gamma': 3.6363679380019898, 'learning_rate': 0.030994976963734877, 'max_depth': 8, 'min_child_weight': 6, 'n_estimators': 70, 'reg_alpha': 0.4272930587596615, 'reg_lambda': 0.00026176266364324704, 'subsample': 0.8144715637487802, 'tree_method': 'hist'}\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8472  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8472  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.8501  | 0.8000 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.8436  | 0.8182 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8477  | 0.8092 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.8687  | 0.8108 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.8222  | 0.8077 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9267  | 0.8673 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.9098  | 0.8538 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "xgb_search_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 300, 5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 10, 1)),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.00001), np.log(100)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.0001), np.log(1.)),\n",
    "    \"booster\": hp.choice('booster', ['gbtree', 'gblinear']),\n",
    "    'tree_method': 'hist',\n",
    "    'gamma': hp.uniform('gamma', 0., 5.)\n",
    "}\n",
    "\n",
    "def xgb_objective(params):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_child_weight=params['min_child_weight'], \n",
    "        subsample=params['subsample'], \n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        booster=params['booster'],\n",
    "        tree_method=params['tree_method'],\n",
    "        gamma=params['gamma'],\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_valid_hat)\n",
    "    return -f1\n",
    "\n",
    "best_xgb_params = fmin(\n",
    "    fn=xgb_objective,\n",
    "    space=xgb_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_xgb_params = space_eval(xgb_search_space, best_xgb_params)\n",
    "\n",
    "model = XGBClassifier(**best_xgb_params)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_train_score = model.predict_proba(X_merge)[:, 1]\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_score = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = classification_metrics(y_merge, y_train_pred, y_train_score)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred, y_test_score)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "print('XGBoost Classifier results:')\n",
    "print(f'Best params: {best_xgb_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_xgb.txt', 'w') as file:\n",
    "    file.write(f'BACE classfication\\n')\n",
    "    file.write('XGBoost Classifier results:\\n')\n",
    "    file.write(f'Best params: {best_xgb_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77815648",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b55b829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/g6vxcdsx4jxbw4lh9cth689c0000gn/T/ipykernel_8803/842798693.py:83: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  torch.FloatTensor(X_train),\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:37<00:00,  1.02trial/s, best loss: -0.8297872340425532]\n",
      "ANN Classifier results:\n",
      "Best params: {'activation': 'gelu', 'batch_size': 128, 'dropout_rate': 0.136465625720233, 'epochs': 100, 'hidden_dim_1': 32, 'hidden_dim_2': 64, 'hidden_dim_3': 32, 'hidden_dim_4': 32, 'learning_rate': 0.00881019811353148, 'n_layers': 3, 'patience': 10}\n",
      "+-------------------+---------+--------+\n",
      "| Metrics           | Train   | Test   |\n",
      "+===================+=========+========+\n",
      "| Accuracy          | 0.8604  | 0.8224 |\n",
      "+-------------------+---------+--------+\n",
      "| Recall            |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall recall    | 0.8604  | 0.8224 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 recall    | 0.8487  | 0.7867 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 recall    | 0.8746  | 0.8571 |\n",
      "+-------------------+---------+--------+\n",
      "| Precision         |         |        |\n",
      "+-------------------+---------+--------+\n",
      "| Overall precision | 0.8621  | 0.8236 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 0 precision | 0.8917  | 0.8429 |\n",
      "+-------------------+---------+--------+\n",
      "| Class 1 precision | 0.8262  | 0.8049 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-ROC           | 0.9349  | 0.8931 |\n",
      "+-------------------+---------+--------+\n",
      "| AUC-PRC           | 0.9191  | 0.8831 |\n",
      "+-------------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class FCNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            hidden_dim_1=128,\n",
    "            hidden_dim_2=128,\n",
    "            hidden_dim_3=128,\n",
    "            hidden_dim_4=128,\n",
    "            dropout_rate=0.2,\n",
    "            activation='relu',\n",
    "            n_layers=4\n",
    "    ):\n",
    "        super(FCNClassifier, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2, bias=True)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3, bias=True)\n",
    "        self.fc4 = nn.Linear(hidden_dim_3, hidden_dim_4, bias=True)\n",
    "        \n",
    "        output_dims = [hidden_dim_1, hidden_dim_2, hidden_dim_3, hidden_dim_4]\n",
    "        last_hidden_dim = output_dims[n_layers-1]\n",
    "        self.out = nn.Linear(last_hidden_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)                     # Layer 1\n",
    "        x = self._activation(x)\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        if self.n_layers >= 2:              # Layer 2 (if applicable)\n",
    "            x = self.fc2(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        if self.n_layers >= 3:              # Layer 3 (if applicable)\n",
    "            x = self.fc3(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if self.n_layers >= 4:              # Layer 4 (if applicable)\n",
    "            x = self.fc4(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'gelu':\n",
    "            return F.gelu(x)\n",
    "        elif self.activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        elif self.activation == 'selu':\n",
    "            return F.selu(x)\n",
    "        else:\n",
    "            return F.relu(x)\n",
    "    \n",
    "def train_ann(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        learning_rate = 0.001,\n",
    "        batch_size=128,\n",
    "        epochs=100,\n",
    "        patience=10,\n",
    "        device='cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    early_stopping = True if X_valid is not None and y_valid is not None else False\n",
    "    best_f1 = 0\n",
    "    no_improvement_count = 0\n",
    "    best_state = None\n",
    "    best_num_epochs = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Valid\n",
    "        if early_stopping:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
    "                y_valid_out = model(X_valid_tensor).cpu().numpy().flatten()\n",
    "                y_valid_pred = (y_valid_out > 0.5).astype(int)\n",
    "                f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                no_improvement_count = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                best_num_epochs = epoch + 1\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= patience:\n",
    "                    break\n",
    "    \n",
    "    if early_stopping and best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        return model, best_num_epochs\n",
    "        \n",
    "    return model, epochs\n",
    "\n",
    "def predict_test(model, X, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        predictions = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def ann_objective(\n",
    "        params,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        input_dim,\n",
    "        device='cpu'\n",
    "):\n",
    "    model = FCNClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim_1=params['hidden_dim_1'],\n",
    "        hidden_dim_2=params['hidden_dim_2'],\n",
    "        hidden_dim_3=params['hidden_dim_3'],\n",
    "        hidden_dim_4=params['hidden_dim_4'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        activation=params['activation'],\n",
    "        n_layers=params['n_layers']\n",
    "    )\n",
    "\n",
    "    model, best_num_epoch = train_ann(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        epochs=params['epochs'],\n",
    "        patience=params['patience'],\n",
    "        device=device,\n",
    "        batch_size=params['batch_size']\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
    "        y_valid_out = model(X_valid_tensor).cpu().numpy().flatten()\n",
    "        y_valid_pred = (y_valid_out > 0.5).astype(int)\n",
    "        f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "    return {\n",
    "        'loss': -f1,\n",
    "        'status': 'ok',\n",
    "        'best_num_epoch': best_num_epoch\n",
    "    }\n",
    "\n",
    "ann_search_space = {\n",
    "    'n_layers': scope.int(hp.quniform('n_layers', 1, 4, 1)),\n",
    "    'hidden_dim_1': scope.int(hp.quniform('hidden_dim_1', 32, 192, 32)),\n",
    "    'hidden_dim_2': scope.int(hp.quniform('hidden_dim_2', 32, 192, 32)),\n",
    "    'hidden_dim_3': scope.int(hp.quniform('hidden_dim_3', 32, 192, 32)),\n",
    "    'hidden_dim_4': scope.int(hp.quniform('hidden_dim_4', 32, 192, 32)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'batch_size': scope.int(hp.quniform('batch_size', 32, 128, 16)),\n",
    "    'epochs': 100,\n",
    "    'patience': 10,\n",
    "    'activation': hp.choice('activation', ['relu', 'selu', 'elu', 'gelu'])\n",
    "}\n",
    "\n",
    "input_dim = len(X_train[0])\n",
    "objective_fn = lambda params: ann_objective(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    input_dim\n",
    ")\n",
    "trials = Trials()\n",
    "best_ann_params = fmin(\n",
    "    fn=objective_fn,\n",
    "    space=ann_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_ann_params = space_eval(ann_search_space, best_ann_params)\n",
    "\n",
    "model = FCNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim_1=best_ann_params['hidden_dim_1'],\n",
    "    hidden_dim_2=best_ann_params['hidden_dim_2'],\n",
    "    hidden_dim_3=best_ann_params['hidden_dim_3'],\n",
    "    hidden_dim_4=best_ann_params['hidden_dim_4'],\n",
    "    dropout_rate=best_ann_params['dropout_rate'],\n",
    "    activation=best_ann_params['activation'],\n",
    "    n_layers=best_ann_params['n_layers']\n",
    ")\n",
    "\n",
    "best_trial = trials.best_trial\n",
    "best_num_epochs = best_trial['result']['best_num_epoch']\n",
    "\n",
    "model, _ = train_ann(\n",
    "    model,\n",
    "    X_merge,\n",
    "    y_merge,\n",
    "    X_valid=None,\n",
    "    y_valid=None,\n",
    "    learning_rate=best_ann_params['learning_rate'],\n",
    "    batch_size=best_ann_params['batch_size'],\n",
    "    epochs=best_num_epochs,\n",
    ")\n",
    "\n",
    "y_train_score = predict_test(model, X_merge)\n",
    "y_train_pred = (y_train_score > 0.5).astype(int)\n",
    "\n",
    "y_test_score = predict_test(model, X_test)\n",
    "y_test_pred = (y_test_score > 0.5).astype(int)\n",
    "\n",
    "train_metrics = classification_metrics(y_merge, y_train_pred, y_train_score)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred, y_test_score)\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"Accuracy\", f'{train_metrics['accuracy']:.4f}', f'{test_metrics['accuracy']:.4f}'],\n",
    "    [\"Recall\"],\n",
    "    [\"Overall recall\", f'{train_metrics['recall']:.4f}', f'{test_metrics['recall']:.4f}'],\n",
    "    [\"Class 0 recall\", f'{train_metrics['0_recall']:.4f}', f'{test_metrics['0_recall']:.4f}'],\n",
    "    [\"Class 1 recall\", f'{train_metrics['1_recall']:.4f}', f'{test_metrics['1_recall']:.4f}'],\n",
    "    [\"Precision\", '', ''],\n",
    "    [\"Overall precision\", f'{train_metrics['precision']:.4f}', f'{test_metrics['precision']:.4f}'],\n",
    "    [\"Class 0 precision\", f'{train_metrics['0_precision']:.4f}', f'{test_metrics['0_precision']:.4f}'],\n",
    "    [\"Class 1 precision\", f'{train_metrics['1_precision']:.4f}', f'{test_metrics['1_precision']:.4f}'],\n",
    "    [\"AUC-ROC\", f'{train_metrics['auc-roc']:.4f}', f'{test_metrics['auc-roc']:.4f}'],\n",
    "    [\"AUC-PRC\", f'{train_metrics['auc-prc']:.4f}', f'{test_metrics['auc-prc']:.4f}'],\n",
    "]\n",
    "\n",
    "print('ANN Classifier results:')\n",
    "print(f'Best params: {best_ann_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_ann.txt', 'w') as file:\n",
    "    file.write(f'BACE classfication\\n')\n",
    "    file.write('ANN Classifier results:\\n')\n",
    "    file.write(f'Best params: {best_ann_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
