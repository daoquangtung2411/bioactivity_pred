{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65aae650",
   "metadata": {},
   "source": [
    "## Extended Reduced Graph fingerprint for bioactivity predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3931e9",
   "metadata": {},
   "source": [
    "##### Import libraries\n",
    "\n",
    "- Helper function: load, split dataset, generate fingerprint\n",
    "\n",
    "- Load model from scikit-learn, torch\n",
    "\n",
    "- Load hyperopt module for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706621d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from helper.load_dataset import load_bace_regression\n",
    "from helper.preprocess import split_train_valid_test\n",
    "from helper.features import smi_erg\n",
    "from helper.cal_metrics import regression_metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from hyperopt import hp, tpe, fmin, Trials, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59718e7",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a60163",
   "metadata": {},
   "source": [
    "##### Random Splitting - BACE Regression Task\n",
    "\n",
    "- Load, split dataset\n",
    "\n",
    "- Generate fingerprint, defined target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6479585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "bace_class = load_bace_regression()\n",
    "\n",
    "# Split dataset\n",
    "train, valid, test = split_train_valid_test(bace_class)\n",
    "merge = pd.concat((train, valid))\n",
    "\n",
    "# Generate fingerprint\n",
    "train_smis = train['SMILES']\n",
    "valid_smis = valid['SMILES']\n",
    "test_smis = test['SMILES']\n",
    "merge_smis = merge['SMILES']\n",
    "X_train = [smi_erg(smi) for smi in train_smis]\n",
    "X_valid = [smi_erg(smi) for smi in valid_smis]\n",
    "X_test = [smi_erg(smi) for smi in test_smis]\n",
    "X_merge = [smi_erg(smi) for smi in merge_smis]\n",
    "\n",
    "\n",
    "# Target defined\n",
    "y_train = train['pIC50']\n",
    "y_valid = valid['pIC50']\n",
    "y_test = test['pIC50']\n",
    "y_merge = merge['pIC50']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccac410",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning and model training\n",
    "\n",
    "- Pipeline: Hyperparameter tuning on train and valid set -> Train best params on train + valid -> Test on test set\n",
    "\n",
    "- Model to try:\n",
    "\n",
    "    - Support Vector Machine\n",
    "    - Random Forest\n",
    "    - XGBoost\n",
    "    - Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa508a9",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d3c9086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:52<00:00,  1.92trial/s, best loss: 0.46090333910486203]\n",
      "Random Forest results:\n",
      "Best params: {'max_depth': 10, 'max_features': 115, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 65}\n",
      "+------------+---------+--------+\n",
      "| Metrics    |   Train |   Test |\n",
      "+============+=========+========+\n",
      "| RMSE       |  0.5705 | 0.6969 |\n",
      "+------------+---------+--------+\n",
      "| R2         |  0.819  | 0.7336 |\n",
      "+------------+---------+--------+\n",
      "| MAPE       |  0.0731 | 0.0902 |\n",
      "+------------+---------+--------+\n",
      "| Pearson R  |  0.9121 | 0.8616 |\n",
      "+------------+---------+--------+\n",
      "| Spearman R |  0.8963 | 0.7881 |\n",
      "+------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "rf_search_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 100, 5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 2, 20, 1)),\n",
    "    'max_features': scope.int(hp.quniform('max_features', 5, 150, 5)),\n",
    "    'min_samples_leaf': scope.int(hp.quniform('min_samples_leaf', 2, 20, 1)),\n",
    "    'min_samples_split': scope.int(hp.quniform('min_samples_split', 2, 20, 1))\n",
    "}\n",
    "\n",
    "def rf_objective(params):\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth'], \n",
    "        max_features=params['max_features'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        n_jobs=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_valid_hat)\n",
    "    return mse\n",
    "\n",
    "best_rf_params = fmin(\n",
    "    fn=rf_objective,\n",
    "    space=rf_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_rf_params = space_eval(rf_search_space, best_rf_params)\n",
    "\n",
    "model = RandomForestRegressor(**best_rf_params, random_state=42)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = regression_metrics(y_merge, y_train_pred)\n",
    "test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"RMSE\", f'{train_metrics['rmse']:.4f}', f'{test_metrics['rmse']:.4f}'],\n",
    "    [\"R2\", f'{train_metrics['r2']:.4f}', f'{test_metrics['r2']:.4f}'],\n",
    "    [\"MAPE\", f'{train_metrics['mape']:.4f}', f'{test_metrics['mape']:.4f}'],\n",
    "    [\"Pearson R\", f'{train_metrics['pearsonr']:.4f}', f'{test_metrics['pearsonr']:.4f}'],\n",
    "    [\"Spearman R\", f'{train_metrics['spearmanr']:.4f}', f'{test_metrics['spearmanr']:.4f}'],\n",
    "]\n",
    "\n",
    "print('Random Forest results:')\n",
    "print(f'Best params: {best_rf_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_regress_random/bace_regress_rf.txt', 'w') as file:\n",
    "    file.write(f'BACE Regression\\n')\n",
    "    file.write('Random Forest results:\\n')\n",
    "    file.write(f'Best params: {best_rf_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15e97d",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8d1ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:00<00:00,  1.65trial/s, best loss: 0.49656050745772995]\n",
      "Support Vector Machine results:\n",
      "Best params: {'C': 2.91264342782912, 'epsilon': 0.1430419338690253, 'kernel': 'rbf'}\n",
      "+------------+---------+--------+\n",
      "| Metrics    |   Train |   Test |\n",
      "+============+=========+========+\n",
      "| RMSE       |  0.6428 | 0.6753 |\n",
      "+------------+---------+--------+\n",
      "| R2         |  0.7702 | 0.7498 |\n",
      "+------------+---------+--------+\n",
      "| MAPE       |  0.0762 | 0.0865 |\n",
      "+------------+---------+--------+\n",
      "| Pearson R  |  0.8794 | 0.8668 |\n",
      "+------------+---------+--------+\n",
      "| Spearman R |  0.8597 | 0.8041 |\n",
      "+------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "svm_search_space = {\n",
    "    'C': hp.loguniform('C', np.log(0.1), np.log(10)),\n",
    "    'epsilon': hp.uniform('epsilon', 0.01, 0.2),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly'])\n",
    "}\n",
    "\n",
    "def svm_objective(params):\n",
    "    model = SVR(\n",
    "        C=params['C'],\n",
    "        epsilon=params['epsilon'], \n",
    "        kernel=params['kernel'])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_valid_hat)\n",
    "    return mse\n",
    "\n",
    "best_svm_params = fmin(\n",
    "    fn=svm_objective,\n",
    "    space=svm_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_svm_params = space_eval(svm_search_space, best_svm_params)\n",
    "\n",
    "model = SVR(**best_svm_params)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = regression_metrics(y_merge, y_train_pred)\n",
    "test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"RMSE\", f'{train_metrics['rmse']:.4f}', f'{test_metrics['rmse']:.4f}'],\n",
    "    [\"R2\", f'{train_metrics['r2']:.4f}', f'{test_metrics['r2']:.4f}'],\n",
    "    [\"MAPE\", f'{train_metrics['mape']:.4f}', f'{test_metrics['mape']:.4f}'],\n",
    "    [\"Pearson R\", f'{train_metrics['pearsonr']:.4f}', f'{test_metrics['pearsonr']:.4f}'],\n",
    "    [\"Spearman R\", f'{train_metrics['spearmanr']:.4f}', f'{test_metrics['spearmanr']:.4f}'],\n",
    "]\n",
    "\n",
    "print('Support Vector Machine results:')\n",
    "print(f'Best params: {best_svm_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_regress_svm.txt', 'w') as file:\n",
    "    file.write(f'BACE regression\\n')\n",
    "    file.write('Support Vector Machine results:\\n')\n",
    "    file.write(f'Best params: {best_svm_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fa474",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb31dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.59trial/s, best loss: 0.4653867795471255]\n",
      "XGBoost results:\n",
      "Best params: {'booster': 'gbtree', 'colsample_bytree': 0.7822326368673045, 'gamma': 2.510304987576764, 'learning_rate': 0.020742989541380947, 'max_depth': 6, 'min_child_weight': 5, 'n_estimators': 300, 'reg_alpha': 0.0021693043776675252, 'reg_lambda': 1.029146418673291, 'subsample': 0.4093123036208131, 'tree_method': 'hist'}\n",
      "+------------+---------+--------+\n",
      "| Metrics    |   Train |   Test |\n",
      "+============+=========+========+\n",
      "| RMSE       |  0.6262 | 0.6979 |\n",
      "+------------+---------+--------+\n",
      "| R2         |  0.782  | 0.7328 |\n",
      "+------------+---------+--------+\n",
      "| MAPE       |  0.0812 | 0.0907 |\n",
      "+------------+---------+--------+\n",
      "| Pearson R  |  0.8911 | 0.8603 |\n",
      "+------------+---------+--------+\n",
      "| Spearman R |  0.8713 | 0.798  |\n",
      "+------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning with Hyperopt\n",
    "trials = Trials()\n",
    "\n",
    "xgb_search_space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 5, 300, 5)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 10, 1)),\n",
    "    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 1, 10, 1)),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.4, 1.0),\n",
    "    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 1.0),\n",
    "    \"reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.00001), np.log(100)),\n",
    "    \"reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.001), np.log(1000)),\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.0001), np.log(1.)),\n",
    "    \"booster\": hp.choice('booster', ['gbtree', 'gblinear']),\n",
    "    'tree_method': 'hist',\n",
    "    'gamma': hp.uniform('gamma', 0., 5.)\n",
    "}\n",
    "\n",
    "def xgb_objective(params):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_child_weight=params['min_child_weight'], \n",
    "        subsample=params['subsample'], \n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        booster=params['booster'],\n",
    "        tree_method=params['tree_method'],\n",
    "        gamma=params['gamma'],\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_valid_hat = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, y_valid_hat)\n",
    "    return mse\n",
    "\n",
    "best_xgb_params = fmin(\n",
    "    fn=xgb_objective,\n",
    "    space=xgb_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_xgb_params = space_eval(xgb_search_space, best_xgb_params)\n",
    "\n",
    "model = XGBRegressor(**best_xgb_params)\n",
    "model.fit(X_merge, y_merge)\n",
    "y_train_pred = model.predict(X_merge)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "\n",
    "train_metrics = regression_metrics(y_merge, y_train_pred)\n",
    "test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"RMSE\", f'{train_metrics['rmse']:.4f}', f'{test_metrics['rmse']:.4f}'],\n",
    "    [\"R2\", f'{train_metrics['r2']:.4f}', f'{test_metrics['r2']:.4f}'],\n",
    "    [\"MAPE\", f'{train_metrics['mape']:.4f}', f'{test_metrics['mape']:.4f}'],\n",
    "    [\"Pearson R\", f'{train_metrics['pearsonr']:.4f}', f'{test_metrics['pearsonr']:.4f}'],\n",
    "    [\"Spearman R\", f'{train_metrics['spearmanr']:.4f}', f'{test_metrics['spearmanr']:.4f}'],\n",
    "]\n",
    "\n",
    "\n",
    "print('XGBoost results:')\n",
    "print(f'Best params: {best_xgb_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_regress_xgb.txt', 'w') as file:\n",
    "    file.write(f'BACE regression\\n')\n",
    "    file.write('XGBoost Classifier results:\\n')\n",
    "    file.write(f'Best params: {best_xgb_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77815648",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b55b829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:51<00:00,  2.32s/trial, best loss: 0.5832262040688764]\n",
      "ANN Classifier results:\n",
      "Best params: {'activation': 'gelu', 'batch_size': 80, 'dropout_rate': 0.28705913591237875, 'epochs': 100, 'hidden_dim_1': 128, 'hidden_dim_2': 96, 'hidden_dim_3': 128, 'hidden_dim_4': 96, 'learning_rate': 0.0005795872858795984, 'n_layers': 3, 'patience': 10}\n",
      "+------------+---------+--------+\n",
      "| Metrics    |   Train |   Test |\n",
      "+============+=========+========+\n",
      "| RMSE       |  0.874  | 0.8274 |\n",
      "+------------+---------+--------+\n",
      "| R2         |  0.5753 | 0.6244 |\n",
      "+------------+---------+--------+\n",
      "| MAPE       |  0.1191 | 0.1096 |\n",
      "+------------+---------+--------+\n",
      "| Pearson R  |  0.7733 | 0.7953 |\n",
      "+------------+---------+--------+\n",
      "| Spearman R |  0.7477 | 0.734  |\n",
      "+------------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "class FCNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            hidden_dim_1=128,\n",
    "            hidden_dim_2=128,\n",
    "            hidden_dim_3=128,\n",
    "            hidden_dim_4=128,\n",
    "            dropout_rate=0.2,\n",
    "            activation='relu',\n",
    "            n_layers=4\n",
    "    ):\n",
    "        super(FCNClassifier, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2, bias=True)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, hidden_dim_3, bias=True)\n",
    "        self.fc4 = nn.Linear(hidden_dim_3, hidden_dim_4, bias=True)\n",
    "        \n",
    "        output_dims = [hidden_dim_1, hidden_dim_2, hidden_dim_3, hidden_dim_4]\n",
    "        last_hidden_dim = output_dims[n_layers-1]\n",
    "        self.out = nn.Linear(last_hidden_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc1(x)                     # Layer 1\n",
    "        x = self._activation(x)\n",
    "        x = self.dropout(x)\n",
    "    \n",
    "        if self.n_layers >= 2:              # Layer 2 (if applicable)\n",
    "            x = self.fc2(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        if self.n_layers >= 3:              # Layer 3 (if applicable)\n",
    "            x = self.fc3(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        if self.n_layers >= 4:              # Layer 4 (if applicable)\n",
    "            x = self.fc4(x)\n",
    "            x = self._activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'gelu':\n",
    "            return F.gelu(x)\n",
    "        elif self.activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        elif self.activation == 'selu':\n",
    "            return F.selu(x)\n",
    "        else:\n",
    "            return F.relu(x)\n",
    "    \n",
    "def train_ann(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        learning_rate = 0.001,\n",
    "        batch_size=128,\n",
    "        epochs=100,\n",
    "        patience=10,\n",
    "        device='cpu'\n",
    "):\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.FloatTensor(X_train),\n",
    "        torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    early_stopping = True if X_valid is not None and y_valid is not None else False\n",
    "    best_mse = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    best_state = None\n",
    "    best_num_epochs = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Valid\n",
    "        if early_stopping:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
    "                y_valid_pred = model(X_valid_tensor).cpu().numpy().flatten()\n",
    "                mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                no_improvement_count = 0\n",
    "                best_state = model.state_dict().copy()\n",
    "                best_num_epochs += 1\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= patience:\n",
    "                    break\n",
    "    \n",
    "    if early_stopping and best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        return model, best_num_epochs\n",
    "        \n",
    "    return model, epochs\n",
    "\n",
    "def predict_test(model, X, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        predictions = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def ann_objective(\n",
    "        params,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        input_dim,\n",
    "        device='cpu'\n",
    "):\n",
    "    model = FCNClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim_1=params['hidden_dim_1'],\n",
    "        hidden_dim_2=params['hidden_dim_2'],\n",
    "        hidden_dim_3=params['hidden_dim_3'],\n",
    "        hidden_dim_4=params['hidden_dim_4'],\n",
    "        dropout_rate=params['dropout_rate'],\n",
    "        activation=params['activation'],\n",
    "        n_layers=params['n_layers']\n",
    "    )\n",
    "\n",
    "    model, best_num_epoch = train_ann(\n",
    "        model,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        epochs=params['epochs'],\n",
    "        patience=params['patience'],\n",
    "        device=device,\n",
    "        batch_size=params['batch_size']\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
    "        y_valid_pred = model(X_valid_tensor).cpu().numpy().flatten()\n",
    "        mse = mean_squared_error(y_valid, y_valid_pred)\n",
    "\n",
    "    return {\n",
    "        'loss': mse,\n",
    "        'status': 'ok',\n",
    "        'best_num_epoch': best_num_epoch\n",
    "    }\n",
    "\n",
    "ann_search_space = {\n",
    "    'n_layers': scope.int(hp.quniform('n_layers', 1, 4, 1)),\n",
    "    'hidden_dim_1': scope.int(hp.quniform('hidden_dim_1', 32, 192, 32)),\n",
    "    'hidden_dim_2': scope.int(hp.quniform('hidden_dim_2', 32, 192, 32)),\n",
    "    'hidden_dim_3': scope.int(hp.quniform('hidden_dim_3', 32, 192, 32)),\n",
    "    'hidden_dim_4': scope.int(hp.quniform('hidden_dim_4', 32, 192, 32)),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
    "    'batch_size': scope.int(hp.quniform('batch_size', 32, 128, 16)),\n",
    "    'epochs': 100,\n",
    "    'patience': 10,\n",
    "    'activation': hp.choice('activation', ['relu', 'selu', 'elu', 'gelu'])\n",
    "}\n",
    "\n",
    "input_dim = len(X_train[0])\n",
    "objective_fn = lambda params: ann_objective(\n",
    "    params,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    input_dim\n",
    ")\n",
    "\n",
    "best_ann_params = fmin(\n",
    "    fn=objective_fn,\n",
    "    space=ann_search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "best_ann_params = space_eval(ann_search_space, best_ann_params)\n",
    "\n",
    "model = FCNClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim_1=best_ann_params['hidden_dim_1'],\n",
    "    hidden_dim_2=best_ann_params['hidden_dim_2'],\n",
    "    hidden_dim_3=best_ann_params['hidden_dim_3'],\n",
    "    hidden_dim_4=best_ann_params['hidden_dim_4'],\n",
    "    dropout_rate=best_ann_params['dropout_rate'],\n",
    "    activation=best_ann_params['activation'],\n",
    "    n_layers=best_ann_params['n_layers']\n",
    ")\n",
    "\n",
    "best_trial = trials.best_trial\n",
    "best_num_epochs = best_trial['result']['best_num_epoch']\n",
    "\n",
    "model, _ = train_ann(\n",
    "    model,\n",
    "    X_merge,\n",
    "    y_merge,\n",
    "    X_valid=None,\n",
    "    y_valid=None,\n",
    "    learning_rate=best_ann_params['learning_rate'],\n",
    "    batch_size=best_ann_params['batch_size'],\n",
    "    epochs=best_num_epochs,\n",
    ")\n",
    "\n",
    "y_train_pred = predict_test(model, X_merge)\n",
    "y_test_pred = predict_test(model, X_test)\n",
    "\n",
    "train_metrics = regression_metrics(y_merge, y_train_pred)\n",
    "test_metrics = regression_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print\n",
    "\n",
    "result_header = ['Metrics', 'Train', 'Test']\n",
    "result_body = [\n",
    "    [\"RMSE\", f'{train_metrics['rmse']:.4f}', f'{test_metrics['rmse']:.4f}'],\n",
    "    [\"R2\", f'{train_metrics['r2']:.4f}', f'{test_metrics['r2']:.4f}'],\n",
    "    [\"MAPE\", f'{train_metrics['mape']:.4f}', f'{test_metrics['mape']:.4f}'],\n",
    "    [\"Pearson R\", f'{train_metrics['pearsonr']:.4f}', f'{test_metrics['pearsonr']:.4f}'],\n",
    "    [\"Spearman R\", f'{train_metrics['spearmanr']:.4f}', f'{test_metrics['spearmanr']:.4f}'],\n",
    "]\n",
    "\n",
    "print('ANN Classifier results:')\n",
    "print(f'Best params: {best_ann_params}')\n",
    "print(tabulate(result_body, headers=result_header, tablefmt='grid'))\n",
    "\n",
    "with open('results/bace_class_ann.txt', 'w') as file:\n",
    "    file.write(f'BACE classfication\\n')\n",
    "    file.write('ANN Classifier results:\\n')\n",
    "    file.write(f'Best params: {best_ann_params}')\n",
    "    file.write(tabulate(result_body, headers=result_header, tablefmt='grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
